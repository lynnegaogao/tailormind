[{"importance": 5, "level": 8, "milestone": "Convolutional Networks", "start": 0, "subknowledge": [{"content": "Backpropagation is a method used to calculate the gradient of the loss function with respect to the weights in the network.", "importance": 4, "knowledge": "Backpropagation", "level": 5}, {"content": "Activation functions introduce non-linearity into the output of a neuron.", "importance": 3, "knowledge": "Activation Functions", "level": 5}, {"content": "ReLU stands for Rectified Linear Unit and is a popular activation function that allows models to solve non-linear problems.", "importance": 3, "knowledge": "ReLU", "level": 3}, {"content": "Pooling reduces the dimensionality of each feature map but retains the most important information.", "importance": 3, "knowledge": "Pooling", "level": 3}]}, {"importance": 3, "level": 5, "milestone": "Gradient-Based Learning", "start": 0.3, "subknowledge": [{"content": "Batch Normalization is a technique to provide any layer in a neural network with inputs that are zero mean/unit variance.", "importance": 4, "knowledge": "Batch Normalization", "level": 3}, {"content": "Dropout is a regularization technique that prevents overfitting by dropping out units in neural networks.", "importance": 4, "knowledge": "Dropout", "level": 3}, {"content": "The Adam Optimizer combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.", "importance": 3, "knowledge": "Adam Optimizer", "level": 1}, {"content": "RMSProp is an optimization algorithm designed to address Adagrad's radically diminishing learning rates.", "importance": 3, "knowledge": "RMSProp", "level": 1}]}, {"importance": 2, "level": 1, "milestone": "Data Augmentation", "start": 0.4, "subknowledge": [{"content": "Techniques like rotation, flipping, and cropping are used to artificially expand the dataset.", "importance": 3, "knowledge": "Image Augmentation", "level": 2}, {"content": "Methods include swapping word order, inserting random words, or deleting some words to create variations.", "importance": 3, "knowledge": "Text Augmentation", "level": 2}, {"content": "Involves changing pitch, speed, and adding noise to make models robust in audio recognition tasks.", "importance": 3, "knowledge": "Audio Augmentation", "level": 2}]}, {"importance": 5, "level": 7, "milestone": "Dropout", "start": 0.5, "subknowledge": [{"content": "To reduce overfitting by preventing complex co-adaptations on training data.", "importance": 3, "knowledge": "Purpose of Dropout", "level": 1}, {"content": "Determines the likelihood of neurons being dropped out during training, commonly set to 0.5.", "importance": 4, "knowledge": "Probability in Dropout", "level": 2}, {"content": "Used during training to drop units; during testing, all units are used but with their outputs weighted by the dropout probability.", "importance": 3, "knowledge": "Usage in Training and Testing", "level": 4}]}, {"importance": 3, "level": 3, "milestone": "Deep Belief Networks", "start": 0.8, "subknowledge": [{"content": "RBMs are stochastic neural networks that can learn a probability distribution over its set of inputs.", "importance": 4, "knowledge": "Restricted Boltzmann Machine (RBM)", "level": 1}, {"content": "Involves the use of training data for effective network parametrization and learning fine-tuned parameters.", "importance": 4, "knowledge": "Training Data and Parameter Learning", "level": 3}, {"content": "A method used to approximate complex probabilistic models in RBMs and other networks.", "importance": 3, "knowledge": "Variational Inference", "level": 1}]}]