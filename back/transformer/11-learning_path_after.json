[{"importance": 5, "level": 1, "milestone": "Understanding Basic Concepts", "start": 0, "subknowledge": [{"content": "Inputs are the initial data or features that are fed into a model for processing.", "importance": 5, "knowledge": "Inputs", "level": 1}, {"content": "Outputs are the results produced by a model after processing the inputs.", "importance": 5, "knowledge": "Outputs", "level": 1}, {"content": "Normalization involves adjusting the scale of data attributes or features to a common scale.", "importance": 4, "knowledge": "Normalization", "level": 1}, {"content": "Positional Information in models indicates the location of data points in relation to one another.", "importance": 3, "knowledge": "Positional Information", "level": 1}]}, {"importance": 5, "level": 2, "milestone": "Vectors and Math Foundations", "start": 0.2, "subknowledge": [{"content": "A Query Vector is part of an attention mechanism, representing the query in similarity calculations.", "importance": 5, "knowledge": "Query Vector", "level": 2}, {"content": "A Key Vector is part of an attention mechanism, representing keys for comparison.", "importance": 5, "knowledge": "Key Vector", "level": 2}, {"content": "A Value Vector is part of an attention mechanism, representing the value to retrieve information.", "importance": 5, "knowledge": "Value Vector", "level": 2}, {"content": "Softmax Function is used to normalize output in classification models into probability distribution.", "importance": 5, "knowledge": "Softmax Function", "level": 2}, {"content": "Positional Encoding adds information to model inputs concerning the position of data points.", "importance": 4, "knowledge": "Positional Encoding", "level": 2}]}, {"importance": 5, "level": 3, "milestone": "Foundation of Neural Networks", "start": 0.4, "subknowledge": [{"content": "Neural networks are computing systems vaguely inspired by the biological neural networks.", "importance": 5, "knowledge": "Neural Networks", "level": 3}, {"content": "A method for computing similarity weighted sums of values in attention mechanisms.", "importance": 5, "knowledge": "Scaled Dot-Product Attention", "level": 3}, {"content": "Self-Attention is a mechanism allowing inputs to interact with each other and identify relevant information.", "importance": 5, "knowledge": "Self-Attention", "level": 3}, {"content": "Position Embeddings are used to preserve sequence order in models without recurrence or convolution.", "importance": 4, "knowledge": "Position Embedding", "level": 3}, {"content": "Generative Pre-trained Transformer (GPT) is a model that uses self-attention to generate text.", "importance": 4, "knowledge": "GPT", "level": 3}, {"content": "Bidirectional Encoders process data from both past and future contexts for understanding.", "importance": 4, "knowledge": "Bidirectional Encoder", "level": 3}]}, {"importance": 5, "level": 4, "milestone": "Delving into Model Architectures", "start": 0.6, "subknowledge": [{"content": "Attention Mechanism allows models to focus on relevant parts of the input sequence.", "importance": 5, "knowledge": "Attention Mechanism", "level": 4}, {"content": "Transformers are models that use attention mechanisms to boost speed and understanding of sequences.", "importance": 5, "knowledge": "Transformer Model", "level": 4}, {"content": "Residual Connections in neural networks allow for deeper models by enabling alternate shortcut paths for gradient.", "importance": 5, "knowledge": "Residual Connections", "level": 4}, {"content": "Inception Networks involve modules that allow the network to choose from operations at each step.", "importance": 4, "knowledge": "Inception Networks", "level": 4}]}]