[
    "Self-attention",
    "Multi-head Self-Attention",
    "Layer Normalization",
    "Residual Connections",
    "Attention logit scaling",
    "Transformer Encoder",
    "Transformer Decoder",
    "Transformer Encoder-Decoder",
    "Machine Learning",
    "Deep Learning",
    "Autoregressive Language Model",
    "Softmax",
    "Linear Transformation",
    "Future Masking in Decoders",
    "Batched Softmax Operation",
    "Scaled Dot Product Attention"
]