["Self-attention", "Transformer architecture", "Neural architectures for NLP", "Key-query-value mechanism", "Contextual representations", "Position representations", "Multi-head Self-Attention", "Layer normalization", "Residual connections", "Attention scaling", "Transformer Encoder-Decoder", "Cross-Attention"]