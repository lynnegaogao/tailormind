[[["Self-attention", "Attention Mechanism", "Transformer Model"], ["Self-attention", "Positional Encoding", "Positional Embedding"], ["Self-attention", "Multi-head Attention", "Heads"], ["Self-attention", "Scaling Factor", "Dropout"], ["Self-attention", "Layer Normalization", "Layer Normalization Layer"]], [["Neural Networks", "Attention Mechanism", "Multi-head Self-Attention"], ["Neural Networks", "Attention Mechanism", "Scaled Dot-Product Attention"], ["Neural Networks", "Attention Mechanism", "Concatenated Attention"], ["Neural Networks", "Attention Mechanism", "Key-Value Attention"], ["Neural Networks", "Attention Mechanism", "Position-wise Attention"]], [["Layer Normalization", "Normalization", "Layer"], ["Layer Normalization", "Normalization", "Batch"], ["Layer Normalization", "Normalization", "Input"], ["Layer Normalization", "Normalization", "Output"], ["Layer Normalization", "Normalization", "Layer Size"], ["Layer Normalization", "Normalization", "Epsilon"], ["Layer Normalization", "Normalization", "Beta"], ["Layer Normalization", "Normalization", "Gamma"], ["Layer Normalization", "Normalization", "Scale"], ["Layer Normalization", "Normalization", "Offset"]], [["Attention", "Logits", "Scaling"], ["Attention", "Scaled Logits", "Softmax"], ["Attention", "Softmax Output", "Probabilities"], ["Attention", "Probabilities", "Weights"], ["Attention", "Weights", "Context Vector"]], [["Transformer Encoder", "Self Attention Mechanism", "Positional Encoding"], ["Transformer Encoder", "Positionwise Feed-Forward Network", "Layer Normalization"], ["Transformer Encoder", "Embedding Layer", "Word Embeddings"], ["Transformer Encoder", "Embedding Layer", "Position Embeddings"], ["Transformer Encoder", "Encoder Block", "Multi Head Attention"], ["Transformer Encoder", "Encoder Block", "Feed Forward Network"], ["Transformer Encoder", "Decoder Block", "Masked Multi Head Attention"], ["Transformer Encoder", "Decoder Block", "Output Layer"], ["Transformer Encoder", "Decoder Block", "Logits"], ["Transformer Encoder", "Decoder Block", "Softmax Function"]], [["Transformer Encoder-Decoder", "Encoder", "Self-Attention Mechanism"], ["Transformer Encoder-Decoder", "Decoder", "Masked Language Modeling"], ["Transformer Encoder-Decoder", "Decoder", "Next Sentence Prediction"], ["Transformer Encoder-Decoder", "Decoder", "Machine Translation"], ["Transformer Encoder-Decoder", "Encoder", "Positional Encoding"]], [["Machine Learning", "Supervised Learning", "Regression"], ["Machine Learning", "Supervised Learning", "Classification"], ["Machine Learning", "Unsupervised Learning", "Clustering"], ["Machine Learning", "Unsupervised Learning", "Density Estimation"], ["Machine Learning", "Transfer Learning", "Fine Tuning"], ["Machine Learning", "Transfer Learning", "Pre-trained Models"], ["Machine Learning", "Deep Learning", "Convolutional Neural Networks (CNN)"], ["Machine Learning", "Deep Learning", "Recurrent Neural Networks (RNN)"], ["Machine Learning", "Deep Learning", "Generative Adversarial Networks (GAN)"]], [["Neural Networks", "Activation Functions", "Softmax Function"], ["Deep Learning", "Softmax", "Classification"], ["Machine Learning", "Softmax", "Probabilistic Modeling"], ["Pattern Recognition", "Softmax", "Classifier"], ["Natural Language Processing", "Softmax", "Text Classification"], ["Image Recognition", "Softmax", "Image Classification"]], [["Linear Transformation", "Matrix Multiplication", "Matrix Inversion"], ["Linear Transformation", "Linear Combination", "Vector Addition"], ["Linear Transformation", "Linear Equations", "System of Linear Equations"], ["Linear Transformation", "Eigenvalues", "Eigenvectors"], ["Linear Transformation", "Singular Value Decomposition", "Singular Values"]], [["Neural Networks", "Activation Functions", "Softmax Function"], ["Neural Networks", "Batch Normalization", "Softmax Function"], ["Neural Networks", "Dropout", "Softmax Function"], ["Neural Networks", "Early Stopping", "Softmax Function"], ["Neural Networks", "Gradient Clipping", "Softmax Function"]], [["Attention Mechanism", "Scaled Dot Product Attention", "Self-Attention"], ["Attention Mechanism", "Multi-Head Attention", "Query Attention"], ["Attention Mechanism", "Multi-Head Attention", "Key Attention"], ["Attention Mechanism", "Multi-Head Attention", "Value Attention"], ["Attention Mechanism", "Position-wise Multihead Attention", "Query Attention"], ["Attention Mechanism", "Position-wise Multihead Attention", "Key Attention"], ["Attention Mechanism", "Position-wise Multihead Attention", "Value Attention"]]]