{
    "nodes": [
        {
            "id": "Self-attention",
            "label": "Self-attention",
            "level": 2,
            "size": 4
        },
        {
            "id": "Attention Mechanism",
            "label": "Attention Mechanism",
            "level": 1,
            "size": 5
        },
        {
            "id": "Transformer Model",
            "label": "Transformer Model",
            "level": 3,
            "size": 5
        },
        {
            "id": "Positional Encoding",
            "label": "Positional Encoding",
            "level": 4,
            "size": 3
        },
        {
            "id": "Multi-head Attention",
            "label": "Multi-head Attention",
            "level": 3,
            "size": 4
        },
        {
            "id": "Heads",
            "label": "Heads",
            "level": 7,
            "size": 2
        },
        {
            "id": "Scaling Factor",
            "label": "Scaling Factor",
            "level": 2,
            "size": 3
        },
        {
            "id": "Dropout",
            "label": "Dropout",
            "level": 4,
            "size": 3
        },
        {
            "id": "Layer Normalization",
            "label": "Layer Normalization",
            "level": 4,
            "size": 4
        },
        {
            "id": "Normalization",
            "label": "Normalization",
            "level": 2,
            "size": 3
        },
        {
            "id": "Input",
            "label": "Input",
            "level": 8,
            "size": 1
        },
        {
            "id": "Output",
            "label": "Output",
            "level": 6,
            "size": 2
        },
        {
            "id": "Attention",
            "label": "Attention",
            "level": 1,
            "size": 5
        },
        {
            "id": "Transformer Encoder",
            "label": "Transformer Encoder",
            "level": 3,
            "size": 5
        },
        {
            "id": "Transformer Decoder",
            "label": "Transformer Decoder",
            "level": 3,
            "size": 5
        },
        {
            "id": "Machine Learning",
            "label": "Machine Learning",
            "level": 1,
            "size": 5
        },
        {
            "id": "Neural Networks",
            "label": "Neural Networks",
            "level": 2,
            "size": 4
        },
        {
            "id": "Linear Transformation",
            "label": "Linear Transformation",
            "level": 2,
            "size": 3
        }
    ],
    "links": [
        {
            "source": "Self-attention",
            "target": "Attention Mechanism",
            "relation": "is a type of"
        },
        {
            "source": "Attention Mechanism",
            "target": "Transformer Model",
            "relation": "used in"
        },
        {
            "source": "Self-attention",
            "target": "Positional Encoding",
            "relation": "enhanced by"
        },
        {
            "source": "Self-attention",
            "target": "Multi-head Attention",
            "relation": "extended to"
        },
        {
            "source": "Multi-head Attention",
            "target": "Heads",
            "relation": "consists of"
        },
        {
            "source": "Self-attention",
            "target": "Scaling Factor",
            "relation": "utilizes"
        },
        {
            "source": "Scaling Factor",
            "target": "Dropout",
            "relation": "complemented with"
        },
        {
            "source": "Self-attention",
            "target": "Layer Normalization",
            "relation": "paired with"
        },
        {
            "source": "Layer Normalization",
            "target": "Normalization",
            "relation": "a form of"
        },
        {
            "source": "Normalization",
            "target": "Input",
            "relation": "applied to"
        },
        {
            "source": "Normalization",
            "target": "Output",
            "relation": "leads to"
        },
        {
            "source": "Attention",
            "target": "Attention Mechanism",
            "relation": "encompasses"
        },
        {
            "source": "Transformer Encoder",
            "target": "Transformer Model",
            "relation": "part of"
        },
        {
            "source": "Transformer Decoder",
            "target": "Transformer Model",
            "relation": "part of"
        },
        {
            "source": "Machine Learning",
            "target": "Neural Networks",
            "relation": "includes"
        },
        {
            "source": "Neural Networks",
            "target": "Transformer Model",
            "relation": "power"
        },
        {
            "source": "Linear Transformation",
            "target": "Machine Learning",
            "relation": "foundation for"
        }
    ]
}